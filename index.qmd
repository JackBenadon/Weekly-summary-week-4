---
title: "Weekly Summary Week 4"
author: "Jack Benadon"
title-block-banner: true
title-block-style: default
toc: true
format: html
 #format: pdf
---

---

## Tuesday, Jan 17

::: {.callout-important}
## TIL


Today, I learnt the following concepts in class:

1. Statistical Learning
1. Regression
1. We went in depth on best fit lines and the distance of the point from the line
:::

```{r}
library(tidyverse)
library(ISLR2) #package associated with the text book
library(cowplot)
library(kableExtra)
library(htmlwidgets)
```

## Statistical learning:
Suppose we are given a dataset called the predictor variables/covariates that are denoted by _$X$_.

And the response/outcome variable called _$Y$_.

The goal of statistical learning is to find a function _$f$_ such that _$Y$_=_$F(X)$_.

There are different flavors of statistical learning:

* Supervised learning: _$y,x$_
  - Regression _when $y$ is in R_
  - Classification
* Unsupervised learning: _There is no $y$_
* Semi-supervised learning: _When we have $y$ but $x$ has significantly more observations than $y$_
* Reinforcement learning: _When your model is when the $x$ and $y$ datasets are allowed to change_

### Today we are going to be focusing on regression.
```{r}
url <- "https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt"

df <- read_tsv(url)
df %>% head(.,10) %>% kable
```

#### Our goal is to predict the birth rate as a function of the poverty rate

```{r}
x <- df$PovPct
y <- df$Brth15to17
```

Now we are going to visualize the relationship between $x$ and $y$ with a scatter plot
```{r}

colnames(df) <- tolower(colnames(df))  
plt <- function (){
plot(x,y, pch=20, xlab = "Pov %", ylab = "Birth rate (15-17)")
}
```

#### Lines throught the points: 

```{r}
b0 <- c(0,1,2)
b1 <- c(1,2,3)

par(mfrow = c(3,3))
for (B0 in b0){
  for(B1 in b1){
    plt()
    curve(B0 + B1 * x ,0,30, add=T, col='red')
    title(main = paste("b0 = ", B0, "and b1 = ", B1))
  }
}
```
The distance between the point and the regression line is equal to $y$ - $yhat$

#### Leat squares estimator:
```{r}
b0 <- 10
b1 <-1.1

yhat <- b0 + b1 * x

plt()
curve(b0 +b1 * x, 0 ,30, add = t, col='red')
title(main = paste('b0 = ', b0, ' and b1= ', b1))
segments (x,y,x, yhat)

resids <- abs(y-yhat)^2
ss_resids <- sum(resids)
title(main = paste("b0, b1, ss_residuals = ", b0, b1, ss_resids, sep=', '))

```

#### The best fit line minimizes residuals
```{r}
 model <- lm(y~x)
 sum(residuals(model)^2)
 
 summary(model)
```


## Thursday, Jan 19



::: {.callout-important}
## TIL

Today, I learnt the following concepts in class:

1. Linear models
1. Terminology of regression and linear models
1. Prediction in linear regression
:::

If we want to model $y$ as a function of $x$. We have to do:
```{r}
formula(y ~ x)
```

A liner regression model in r is called using the **L**inear **M**odel _(therefore: lm)_.
```{r}
model <- lm(y~x)
```

### What are the null and alternate hypotheses for a regression model?
Lets think about the objective of this: **We want to find the best linear model to fit $y \sin x$**

The null hypothesis is: 

> There is no linear relationship between $y$ and $x$.
 
 What does it mean in terms of $\beta_0$ and $\beta_1$? **It means that $\beta_1 = 0$** in $H_0$
 
 The alternate hypothesis is that $\beta_1 \neq 0$
 
 **To summarize:**
$$
\begin {align}
H_0: \beta_1 = 0 && H_1: \beta_1 \neq 0
\end {align}
$$
When we see a small $p$ value, then we reject the null hypothesis in fabour of the alternate hypothesis. What is the implication of this w.r.t the original model objective?

> There is a significant relationship between $y$ and $x$. Or, in more mathematical terms, there is significant evidence in favour of a correlation between $x$ and $y$.

This is what the $p$-values in the model output are capturing. We can also use the `kable` function to print out the results nicely.

```{r}
library(broom)
summary(model) %>%
broom::tidy()%>%
knitr::kable()
```


We have the following terminology for different components of the model:

1. Covariate: $x$
```{r}
summary(x)
```

2. Response: $y$
```{r}
head(y)
```

3. Fitted values: $\hat{y}$
```{r}
yhat <- fitted(model)
```

4. Residuals: $e = y - \hat{y}$
```{r}
res <- residuals(model)
head(res)
```

Some other important terms are the following:

1. Sum of squares for residuals:
$SS_{Res} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y-i - \hat{y}_i^2)$

2. Sum of squares for regression:
$SS_{Reg} = \sum_ {i=1}^n e_i^2 (\hat{y}_i - \bar{y})^2$

3. Sum of squares total
$SS_{Tot} = \sum{i=1}^n ({y}_i-\bar{y})^2$

Another important summary in the model output is the $R^2$ value, which is given as follows:
$$
R^2 = \frac{SS_{Reg}}{SS_{Tot}}
$$
Lets have a look at what this means in the following example. Im going to create the following examples:

```{r}
x <- seq(0,5,length = 100)

b0 <- 1
b1 <- 3

y1 <- b0 +b1 * x + rnorm(100)
y2 <- b0 +b1 * x + rnorm(100) *3

par(mfrow=c(1,2))

plot(x,y1)

plot(x,y2)

```
```{r}
model1 <- lm(y1 ~ x )
model2 <- lm(y2 ~ x)

par(mfrow=c(1,2))

plot(x,y1)
curve(
  coef(model1)[1] + coef(model1)[2] *x, add=T, col= "red"
)

plot(x,y2)
curve(
  coef(model2)[1] + coef(model2)[2] *x, add=T, col= "red"
)
```
The summary for model1 is: 
```{r}
summary(model1)
```

The summary for model2 is:
```{r}
summary(model2)
```

Lets go back to the poverty dataset.
```{r}
x <- df$povpct
y <-df$brth15to17
plt()
```

Suppose we have a new state whose `povpct` is $21$.
```{r}
plt()
abline(v =21,col="green")
lines(x,fitted(lm(y~x)), col='red')
```

What could we do to predict the $y$ value for this new state?

our best prediction is going to be in the intersection. In `R` we can use the `predict()` function to do this:

```{r}
new_x <- data.frame(x = c(21))
new_y <- predict(model, new_x)

new_y

plt()
abline(v =21,col="green")
lines(x,fitted(lm(y~x)), col='red')
points(new_x, new_y, col = 'purple')
```







